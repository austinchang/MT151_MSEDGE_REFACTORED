"""
AIæ•´åˆæ¨¡çµ„ - é€ézen-mcp-serveræ•´åˆXAIã€Geminiã€OpenAIæ¨¡å‹

æ­¤æ¨¡çµ„æä¾›èˆ‡zen-mcp-serverçš„æ•´åˆï¼Œå…è¨±MT151_MSEDGEä½¿ç”¨å¤šç¨®AIæ¨¡å‹ä¾†ï¼š
- æ™ºèƒ½åˆ†ææ¸¬è©¦è³‡æ–™
- è‡ªå‹•åŒ–æ±ºç­–å»ºè­°
- éŒ¯èª¤è¨ºæ–·å’Œä¿®å¾©å»ºè­°
- è‡ªç„¶èªè¨€è™•ç†ç”¨æˆ¶è¼¸å…¥
"""

import asyncio
import json
import logging
import subprocess
import sys
import tempfile
import time
from pathlib import Path
from typing import Any, Dict, List, Optional, Union

import requests
from pydantic import BaseModel


class AIModelConfig(BaseModel):
    """AIæ¨¡å‹é…ç½®"""
    model_name: str = "auto"
    provider: str = "auto"  # auto, xai, gemini, openai
    temperature: float = 0.7
    max_tokens: int = 2000
    timeout: int = 30


class AIResponse(BaseModel):
    """AIå›æ‡‰çµæ§‹"""
    success: bool
    content: str
    model_used: str
    token_usage: Optional[Dict[str, int]] = None
    error: Optional[str] = None


class ZenMCPClient:
    """Zen MCP Serverå®¢æˆ¶ç«¯"""
    
    def __init__(self, config: AIModelConfig):
        self.config = config
        self.logger = logging.getLogger(__name__)
        self.zen_server_path = None
        self._find_zen_server()
    
    def _find_zen_server(self):
        """å°‹æ‰¾zen-mcp-serverçš„è·¯å¾‘"""
        potential_paths = [
            Path(__file__).parent.parent.parent / "zen-mcp-server",
            Path.home() / "projects" / "MCP" / "zen-mcp-server",
            Path("/home/tengjung_chang/projects/MCP/zen-mcp-server")
        ]
        
        for path in potential_paths:
            if path.exists() and (path / "server.py").exists():
                self.zen_server_path = path
                self.logger.info(f"æ‰¾åˆ°zen-mcp-server: {path}")
                return
        
        self.logger.warning("æœªæ‰¾åˆ°zen-mcp-serverï¼Œéƒ¨åˆ†AIåŠŸèƒ½å¯èƒ½ç„¡æ³•ä½¿ç”¨")
    
    async def analyze_test_data(self, test_data: Dict[str, Any]) -> AIResponse:
        """ä½¿ç”¨AIåˆ†ææ¸¬è©¦è³‡æ–™"""
        prompt = f"""
        è«‹åˆ†æä»¥ä¸‹æ¸¬è©¦è³‡æ–™çš„åˆç†æ€§å’Œå®Œæ•´æ€§ï¼š
        
        æ¸¬è©¦è³‡æ–™ï¼š
        {json.dumps(test_data, ensure_ascii=False, indent=2)}
        
        è«‹æª¢æŸ¥ï¼š
        1. æ–™è™Ÿæ ¼å¼æ˜¯å¦æ­£ç¢º
        2. ç«™ä½é…ç½®æ˜¯å¦åˆç†
        3. ç‰ˆæœ¬è™Ÿæ ¼å¼æ˜¯å¦ç¬¦åˆè¦ç¯„
        4. æè¿°æ˜¯å¦æ¸…æ™°æ˜ç¢º
        5. æ˜¯å¦æœ‰æ½›åœ¨çš„é…ç½®è¡çª
        
        è«‹æä¾›åˆ†æçµæœå’Œæ”¹å–„å»ºè­°ã€‚
        """
        
        return await self._call_ai_tool("analyze", {
            "content": prompt,
            "focus": "data_validation"
        })
    
    async def suggest_automation_improvements(self, error_log: str) -> AIResponse:
        """æ ¹æ“šéŒ¯èª¤æ—¥èªŒå»ºè­°è‡ªå‹•åŒ–æ”¹å–„"""
        prompt = f"""
        åŸºæ–¼ä»¥ä¸‹éŒ¯èª¤æ—¥èªŒï¼Œè«‹æä¾›è‡ªå‹•åŒ–æ”¹å–„å»ºè­°ï¼š
        
        éŒ¯èª¤æ—¥èªŒï¼š
        {error_log}
        
        è«‹åˆ†æï¼š
        1. éŒ¯èª¤çš„æ ¹æœ¬åŸå› 
        2. å¯èƒ½çš„è§£æ±ºæ–¹æ¡ˆ
        3. é é˜²æ€§æªæ–½
        4. è‡ªå‹•åŒ–æ”¹å–„å»ºè­°
        
        è«‹æä¾›å…·é«”çš„å¯¦æ–½æ­¥é©Ÿã€‚
        """
        
        return await self._call_ai_tool("debug", {
            "issue": prompt,
            "context": "web_automation"
        })
    
    async def generate_test_scenarios(self, base_data: Dict[str, Any]) -> AIResponse:
        """åŸºæ–¼åŸºç¤è³‡æ–™ç”Ÿæˆæ¸¬è©¦å ´æ™¯"""
        prompt = f"""
        åŸºæ–¼ä»¥ä¸‹åŸºç¤æ¸¬è©¦è³‡æ–™ï¼Œè«‹ç”Ÿæˆå¤šå€‹æ¸¬è©¦å ´æ™¯ï¼š
        
        åŸºç¤è³‡æ–™ï¼š
        {json.dumps(base_data, ensure_ascii=False, indent=2)}
        
        è«‹ç”Ÿæˆï¼š
        1. æ­£å¸¸æµç¨‹æ¸¬è©¦å ´æ™¯
        2. é‚Šç•Œæ¢ä»¶æ¸¬è©¦å ´æ™¯
        3. ç•°å¸¸æƒ…æ³æ¸¬è©¦å ´æ™¯
        4. è² è¼‰æ¸¬è©¦å ´æ™¯
        
        æ¯å€‹å ´æ™¯è«‹åŒ…å«å…·é«”çš„æ¸¬è©¦è³‡æ–™å’Œé æœŸçµæœã€‚
        """
        
        return await self._call_ai_tool("testgen", {
            "code": json.dumps(base_data, ensure_ascii=False),
            "focus": "automation_scenarios"
        })
    
    async def chat_with_ai(self, user_message: str, context: Optional[str] = None) -> AIResponse:
        """èˆ‡AIé€²è¡Œå°è©±"""
        full_message = user_message
        if context:
            full_message = f"ä¸Šä¸‹æ–‡ï¼š{context}\n\nç”¨æˆ¶å•é¡Œï¼š{user_message}"
        
        return await self._call_ai_tool("chat", {
            "message": full_message
        })
    
    async def _call_ai_tool(self, tool_name: str, params: Dict[str, Any]) -> AIResponse:
        """èª¿ç”¨AIå·¥å…·"""
        if not self.zen_server_path:
            return AIResponse(
                success=False,
                content="",
                model_used="none",
                error="zen-mcp-serveræœªæ‰¾åˆ°"
            )
        
        try:
            # æº–å‚™èª¿ç”¨zen-mcp-server
            tool_request = {
                "tool": tool_name,
                "parameters": params,
                "model": self.config.model_name,
                "temperature": self.config.temperature,
                "max_tokens": self.config.max_tokens
            }
            
            # ä½¿ç”¨subprocessèª¿ç”¨zen-mcp-server
            result = await self._execute_zen_tool(tool_request)
            
            return AIResponse(
                success=True,
                content=result.get("content", ""),
                model_used=result.get("model", "unknown"),
                token_usage=result.get("token_usage")
            )
            
        except Exception as e:
            self.logger.error(f"AIå·¥å…·èª¿ç”¨å¤±æ•—: {e}")
            return AIResponse(
                success=False,
                content="",
                model_used="unknown",
                error=str(e)
            )
    
    async def _execute_zen_tool(self, tool_request: Dict[str, Any]) -> Dict[str, Any]:
        """åŸ·è¡Œzenå·¥å…·"""
        # é€™è£¡éœ€è¦å¯¦ç¾èˆ‡zen-mcp-serverçš„å¯¦éš›é€šä¿¡
        # æš«æ™‚è¿”å›æ¨¡æ“¬çµæœ
        await asyncio.sleep(0.1)  # æ¨¡æ“¬AIè™•ç†æ™‚é–“
        
        return {
            "content": f"AIåˆ†æçµæœï¼šå·²è™•ç†{tool_request['tool']}è«‹æ±‚",
            "model": self.config.model_name,
            "token_usage": {"prompt_tokens": 100, "completion_tokens": 50}
        }


class AIAssistant:
    """AIåŠ©æ‰‹ä¸»é¡"""
    
    def __init__(self, config: Optional[AIModelConfig] = None):
        self.config = config or AIModelConfig()
        self.client = ZenMCPClient(self.config)
        self.logger = logging.getLogger(__name__)
        
        # è¨­ç½®æ—¥èªŒ
        self._setup_logging()
    
    def _setup_logging(self):
        """è¨­ç½®æ—¥èªŒè¨˜éŒ„"""
        handler = logging.StreamHandler()
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        handler.setFormatter(formatter)
        self.logger.addHandler(handler)
        self.logger.setLevel(logging.INFO)
    
    async def smart_data_validation(self, test_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æ™ºèƒ½è³‡æ–™é©—è­‰"""
        self.logger.info(f"é–‹å§‹æ™ºèƒ½é©—è­‰ {len(test_data)} ç­†æ¸¬è©¦è³‡æ–™")
        
        validation_results = {
            "total_records": len(test_data),
            "valid_records": 0,
            "invalid_records": 0,
            "warnings": [],
            "errors": [],
            "suggestions": []
        }
        
        for i, data in enumerate(test_data):
            try:
                result = await self.client.analyze_test_data(data)
                
                if result.success:
                    # è§£æAIåˆ†æçµæœ
                    if "éŒ¯èª¤" in result.content or "ç„¡æ•ˆ" in result.content:
                        validation_results["invalid_records"] += 1
                        validation_results["errors"].append(f"è¨˜éŒ„ {i+1}: {result.content}")
                    elif "è­¦å‘Š" in result.content or "å»ºè­°" in result.content:
                        validation_results["valid_records"] += 1
                        validation_results["warnings"].append(f"è¨˜éŒ„ {i+1}: {result.content}")
                    else:
                        validation_results["valid_records"] += 1
                else:
                    validation_results["invalid_records"] += 1
                    validation_results["errors"].append(f"è¨˜éŒ„ {i+1}: AIåˆ†æå¤±æ•— - {result.error}")
                    
            except Exception as e:
                validation_results["invalid_records"] += 1
                validation_results["errors"].append(f"è¨˜éŒ„ {i+1}: é©—è­‰ç•°å¸¸ - {str(e)}")
        
        return validation_results
    
    async def suggest_next_action(self, current_context: Dict[str, Any]) -> str:
        """æ ¹æ“šç•¶å‰ä¸Šä¸‹æ–‡å»ºè­°ä¸‹ä¸€æ­¥å‹•ä½œ"""
        context_str = json.dumps(current_context, ensure_ascii=False, indent=2)
        
        result = await self.client.chat_with_ai(
            "æ ¹æ“šç•¶å‰çš„æ“ä½œç‹€æ…‹ï¼Œå»ºè­°æˆ‘ä¸‹ä¸€æ­¥æ‡‰è©²åšä»€éº¼ï¼Ÿ",
            context_str
        )
        
        if result.success:
            return result.content
        else:
            return "AIå»ºè­°æœå‹™æš«æ™‚ä¸å¯ç”¨ï¼Œè«‹æ‰‹å‹•é¸æ“‡ä¸‹ä¸€æ­¥æ“ä½œã€‚"
    
    async def help_with_error(self, error_message: str, context: Optional[str] = None) -> str:
        """å”åŠ©è™•ç†éŒ¯èª¤"""
        result = await self.client.suggest_automation_improvements(
            f"éŒ¯èª¤è¨Šæ¯: {error_message}\nä¸Šä¸‹æ–‡: {context or 'ç„¡'}"
        )
        
        if result.success:
            return result.content
        else:
            return f"ç„¡æ³•ç²å–AIå”åŠ©ï¼ŒåŸå§‹éŒ¯èª¤ï¼š{error_message}"
    
    async def interactive_chat(self):
        """äº’å‹•å¼èŠå¤©"""
        print("ğŸ¤– AIåŠ©æ‰‹å·²æº–å‚™å°±ç·’ï¼è¼¸å…¥ 'quit' é€€å‡ºèŠå¤©ã€‚")
        
        while True:
            try:
                user_input = input("\næ‚¨: ").strip()
                
                if user_input.lower() in ['quit', 'exit', 'é€€å‡º']:
                    print("ğŸ¤– AIåŠ©æ‰‹: å†è¦‹ï¼")
                    break
                
                if not user_input:
                    continue
                
                print("ğŸ¤– AIåŠ©æ‰‹: æ€è€ƒä¸­...")
                result = await self.client.chat_with_ai(user_input)
                
                if result.success:
                    print(f"ğŸ¤– AIåŠ©æ‰‹: {result.content}")
                    if result.model_used != "unknown":
                        print(f"   (ä½¿ç”¨æ¨¡å‹: {result.model_used})")
                else:
                    print(f"ğŸ¤– AIåŠ©æ‰‹: æŠ±æ­‰ï¼Œç™¼ç”ŸéŒ¯èª¤: {result.error}")
                    
            except KeyboardInterrupt:
                print("\nğŸ¤– AIåŠ©æ‰‹: èŠå¤©å·²ä¸­æ–·ï¼Œå†è¦‹ï¼")
                break
            except Exception as e:
                print(f"ğŸ¤– AIåŠ©æ‰‹: ç™¼ç”Ÿç•°å¸¸: {e}")


# ä¾¿åˆ©å‡½æ•¸
async def quick_analyze(data: Dict[str, Any]) -> str:
    """å¿«é€Ÿåˆ†æè³‡æ–™"""
    assistant = AIAssistant()
    result = await assistant.client.analyze_test_data(data)
    return result.content if result.success else f"åˆ†æå¤±æ•—: {result.error}"


async def quick_chat(message: str) -> str:
    """å¿«é€ŸèŠå¤©"""
    assistant = AIAssistant()
    result = await assistant.client.chat_with_ai(message)
    return result.content if result.success else f"èŠå¤©å¤±æ•—: {result.error}"


if __name__ == "__main__":
    # æ¸¬è©¦AIæ•´åˆåŠŸèƒ½
    async def test_ai_integration():
        assistant = AIAssistant()
        
        # æ¸¬è©¦è³‡æ–™åˆ†æ
        test_data = {
            "æ–™è™Ÿ": "C08GL0DIG017A",
            "ç«™ä½": "B/I",
            "ç‰ˆæœ¬": "V3.3.5.9_1.16.0.1E3.12-1",
            "æè¿°": "EN0DIGOA1-0322-GL_HL-325L B/I"
        }
        
        print("æ¸¬è©¦AIè³‡æ–™åˆ†æ...")
        validation_result = await assistant.smart_data_validation([test_data])
        print(f"é©—è­‰çµæœ: {validation_result}")
        
        # æ¸¬è©¦èŠå¤©åŠŸèƒ½
        print("\næ¸¬è©¦AIèŠå¤©åŠŸèƒ½...")
        chat_result = await quick_chat("è«‹ä»‹ç´¹ä¸€ä¸‹MT151_MSEDGEå°ˆæ¡ˆçš„åŠŸèƒ½")
        print(f"AIå›æ‡‰: {chat_result}")
    
    # é‹è¡Œæ¸¬è©¦
    asyncio.run(test_ai_integration())